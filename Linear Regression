library(MPV)
data("table.b4")
attach(table.b4)
T<-table.b4
data<-data.frame(T)
View(data)
dim(data) ## In R there is one row less than the book
mlr<-lm(y~.,data)  ###3.7a and c (t-test is also here)
summary(mlr)
####stackg<-stack(data) #### 3.7b
#stackg
#anova<- aov(values ~ind, data =stackg)
#summary(anova)
#reduced_model<-data.frame(cbind(y,x1,x2,x5,x6,x7,x8,x9))
#stackreduce<-stack(reduced_model)
#View(stackreduce)
#reduced_anova<-aov(values~ind, data=stackreduce)
#summary(reduced_anova)
#all<-c(rep('y',24),rep('x1',24),rep('x2',24),rep('x3',24),rep('x4',24),rep('x5',24),rep('x6',24),rep('x7',24),rep('x8',24),rep('x9',24))
#weight<-c(data$y,data$x1,data$x2,data$x3,data$x4,data$x5,data$x6,data$x7,data$x8,data$x9)
#d<-data.frame(all,weight)
#plot(weight~all,data=data)
#anova<-aov(weight~all,data=data)
#summary(anova)
anova(mlr)#3.7b
reducedmlr<-lm(y~x1+x2+x5+x6+x7+x8+x9)
summary(reducedmlr)
anova(reducedmlr) ##3.7 d ssr(full)-ssr(reduced) = ssr(x3+x4)
library(olsrr)
ols_vif_tol(mlr) ##3.7e

####3.21
x1<- seq(20,50,by=1)
yhat1=100+0.2*x1+4*2 ##x2=2
yhat2=100+0.2*x1+4*8 ##x2=8
yhat3=95+0.15*x1+3*2+x1*2 ##x2=2
yhat4=95+0.15*x1+3*8+x1*8 ##x2=8
plot(x1,yhat1,type="l",col="red")
lines(x1,yhat2,col="green")
###b
yhatb1=100+0.2*x1+4*5 ##x2=5, this model doesn't depend on the specific value of the reaction time.
##c
yhatc1=95+0.15*x1+3*5+x1*5 ##x2=5
yhatc2=95+0.15*x1+3*2+x1*2 ##x2=2
yhatc3=95+0.15*x1+3*8+x1*8 ##x2=8
## for all three equation the model is depending on the value of x2 because of the interaction term. it's changing the slope of the equations.

##### 3.19
table_19 <- read.csv(file.choose(), stringsAsFactors=FALSE)
data<-as.data.frame(table_19)
mlr1<-lm(data$y~data$x2+data$x3+data$x4+data$x5+data$x6+data$x7+data$x8+data$x9)
summary(mlr1)
anova(mlr1)
ols_vif_tol(mlr1)


```{r}
#7.2
x<-c(.25,.50,.75,1,1.25,1.50,1.75,2,2.25,2.50)
y<-c(1.42,1.39,1.55,1.89,2.43,3.15,4.05,5.15,6.43,7.89)
d<-data.frame(y,x)
plot(x,y,main = "scatterplot")
model7.2<-lm(y~poly(x,2),data = d)
#a.y= 3.535+6.53477*x + 2.146377*x^2
summary(aov(model7.2))
#b. from the anova it can be said that F-statistics is 2e-16, that means less than 0.05. so at least one of the coefficients are significant.
#c. H0= beta 2, since the p-value is less than 0.05, we reject the null hypothesis. So beta 2 is significant.
summary(model7.2)
#d.the normality assumption is violated.
qqnorm(resid(model7.2))
```

```{r}
#7.6
y<-c(2.6,2.4,17.32,15.6,16.12,5.36,6.19,10.17,2.62,2.98,6.92,7.06)
x1<-c(31,31,31.5,31.5,31.5,30.5,31.5,30.5,31.5,30.5,31.5,30.5)
x2<-c(21,21,24,24,24,22,22,23,21.5,21.5,22.5,22.5)
d1<-data.frame(y,x1,x2)
model7.6<-lm(d1$y~polym(d1$x1,d1$x2,degree = 2,raw = 2),data =d1 )
summary(model7.6)
summary.aov(model7.6)
#7.6.a. y= -4703.1670+316.8276*x1- 5.1546*x2- 22.8557*x1^2 + 0.1288*x2^2+0.5299*x1x2
#b. Testing significance of regression.  we see the value of F-statistic equals 118.2 with corresponding p value 6.576e-06.Hence we can conclude that our regression model fits the data better than the model with no independent variables.

##c. lack of  fit
#library(alr3)
#pureErrorAnova(model7.6)
#The value of F-statistic equals 0.4605 with corresponding p value 0.72969. On comparing the p-value of the test  for the F-test with the significance level say alpha = 0.05 , we can conclude there is no lack of fit.
#d. p value greater than 0.05. This is not significant and indicates that the interaction term does not contribute significantly to the model.

#e. The p value corresponding to the t statistic for quadratic term x1^2 is greater than the significance level 0.5 The p value corresponding to the t statistic for quadratic term x2^2 greater than the significance level 0.5. so not significant.
```

```{r}
#7.18
y<-c(.222,.395,.422,.437,.428,.467,.444,.378,.494,.456,.452,.112,.432,.101,.232,.306,.0923,.116,.0764,.439,.0944,.117,.0762,.0412,.251,.00002)
x1<-c(7.3,8.7,8.8,8.1,9,8.7,9.3,7.6,10,8.4,9.3,7.7,9.8,7.3,8.5,9.5,7.4,7.8,7.7,10.3,7.8,7.1,7.7,7.4,7.3,7.6)
x2<-c(0,0,.7,4,.5,1.5,2.1,5.1,0,3.7,3.6,2.8,4.2,2.5,2,2.5,2.8,2.8,3,1.7,3.3,3.9,4.3,6,2,7.8)
x3<-c(0,.3,1,.2,1,2.8,1,3.4,.3,4.1,2,7.1,2,6.8,6.6,5,7.8,7.7,8,4.2,8.5,6.6,9.5,10.9,5.2,20.7)
d.7.18<-data.frame(y,x1,x2,x3)
model7.18<-lm(y~x1+x2+x3,data = d.7.18)
summary(model7.18)
summary.aov(model7.18)
qqnorm(resid(model7.18))
#7.18.a y= -0.369046+0.086477*x1 +0.024428*x2 -0.028555 *x3
#b. the global f-test shows that the model is significant. and all the p-vales for t-test are significant for all the coefficients. 
#c. the residual plot shows nearly a straight line. so normality assumption holds.
#d. 
```

```{r}
#7.21.a
x<-c(1700,1720,1730,1740,1750,1760,1770,1780,1790,1795)
y<-c(16,15.8,15.6,15.5,14.8,14,13.5,13,12,11)
d7.21<-data.frame(y,x)
model7.21 <- lm(y~x+I(x^2),data = d7.21)
summary(model7.21)
#7.22 a. the fit is 14.83244  and the Ci for 95% is 14.28449 15.38039. and for 1775 the fit is 13.15297 and CI is between 12.61735 13.68858.
new <- data.frame(x= c(1750,1775))
predict(model7.21, newdata = new)
predict(model7.21, newdata = new, interval = "predict")

```
```{r}
#8.3
n <- read.csv(file.choose(), stringsAsFactors=FALSE)
View(n)

d8.2<-data.frame(n)
d<-d8.2[,-1]
model8.2<-lm(y~x1+x2, data = d)
summary(model8.2)
plot(model8.2)
```

```{r}
#8.16
ex <- read.csv(file.choose(), stringsAsFactors=FALSE)
d8.16<-data.frame(ex)
d<-d8.16[,-1]
d$SURFACE <- as.factor(d$SURFACE)
lm_mod = lm(INHIBIT ~ UVB + SURFACE, data = d)
summary(lm_mod)
plot(lm_mod)
lm_mod1 = lm(INHIBIT ~ UVB + SURFACE, data = d[-17,])
summary(lm_mod1)
plot(lm_mod1)
#there are some outliers like 17th and 16th term. but data points are less so we can't neglect that. boxcox transformation aslo can be applicable because of the variation in the magnitude of explanatory variable. still normal q-q plot shows that linear model can be applicable. adjusted r sq. is .5433 so it needs boxcox transformation. if after doing the transformation adjusted r sq. will not increase then we have to settle with this model accordingly
```

# 9.2
```{r }

library(MPV)
table.b21 <- read.csv(file.choose(), stringsAsFactors=FALSE)
attach(table.b21)
data= table.b21
cor = cor(data[,-(1:2)])
cor
#Correlation exist between x2 and x4. In Othercase high correlation exist between x1 and x3.so, multicollinearity is present.
```


```{r}
model = lm(y~.,data = data[,-1])
library(olsrr)
ols_vif_tol(model)
#Above table represents VIF of four regressor variables.
```



```{r}
x.data = as.matrix(data[,-c(1:2)])
x.data
p = prcomp(x.data, scale= T)
eigenvalue = (p$sdev)^2
eigenvalue
#These are the eigenvalues of four regressors.
```


```{r}
min = min(eigenvalue)
max = max(eigenvalue)
cn = max/min
cn
# Generally, if condition number is less than 100, there is no serious problem with multicolinearity. If it is greater than 1000 there is serious problem with multicolinearity.
```



# 9.7
```{r}
library(MPV)
data("table.b3")
attach(table.b3)
b.3 = table.b3
head(b.3)
dim(b.3)
b.3[,-1]
correlation = cor(b.3[,-1])
correlation
#there is a potential problem with multicollinearity.
```




```{r }
model9.7 = lm(y~.,data = b.3)
library(olsrr)
ols_vif_tol(model9.7)
```

```{r}
xb.3 = as.matrix(b.3[,-1])
xb.3
p = prcomp(na.omit(xb.3), scale= T)
eigenvalue = (p$sdev)^2
eigenvalue
min = min(eigenvalue)
max = max(eigenvalue)
cn = max/min
cn
#The condition number is 2025.239 which is greater than 1000. So, there is a problem with multicolinearity.
```


# 9.17



```{r}

library("ridge")
library("glmnet")
library("lmridge")
library(MPV)
table.b21 <- read.csv(file.choose(), stringsAsFactors=FALSE)
attach(table.b21)
data= table.b21
k = 10^seq(1, -3, by= -0.1)
mod = lmridge(y~., as.data.frame(data[,-1]), k)
plot(mod)
plot(mod, type = 'ridge')
#Above graph shows the ridge trace. Again ridge regression using optimum value of k= 0.0123,
```




```{r}
mod = lmridge(y~., as.data.frame(data[,-1]), 0.0123)
vif(mod)
#VIF values are low and we can say that there is no problem with multicolinearity.

#MSE = 390.481 which is quite high.
```



```{r}
k = 10^seq(1, -3, by= -0.1)
mod3 = lmridge(y~x1+x2, as.data.frame(data[,-1]), k)
plot(mod3)
plot(mod3, type = 'ridge')

mod4 = lmridge(y~., as.data.frame(data[,-1]), 0.0069)
vif(mod4)
coef(mod4, 0.0069)
#Now, we fit a linear regression with 2 regressor and compare the output,
```
```{r}
mod5 = lm(y~x1+x2, data[,-1])
sm = summary(mod5)
sm
sse = sum(sm$residuals^2)
mse = mean(sm$residuals^2)
mse
#MSE is low in case of linear regression. If we compare the coefficient of ridge and linear than the values generated from ridge is low as linear regression.

```
# 9.22 

```{r}

library("MASS")

library("pcr")

library("pls")

library(MPV)
data("table.b3")
attach(table.b3)
b.3 = table.b3
pcr_model = pcr(y~.,data = b.3, scale = TRUE, validation = "CV")
summary(pcr_model)
coef(pcr_model)
validationplot(pcr_model)
validationplot(pcr_model, val.type = "R2")
#Now, applying the LSM,
```
```{r}
mod22 = lm(y~., data = b.3)
summary(mod22)
#There is 10% difference in $R^2$ as compared to least square if we considered two 2 principle components as per the above mentioned graph.Now, Ridge regression using k = 0.0033
```
```{r}

library('ridge')

library("glmnet")

library("lmridge")
k = 10^seq(1, -3, by= -0.1)
mod22r = lmridge(y~., as.data.frame(b.3), k)
plot(mod22r)
plot(mod22r, type = 'ridge')
```

Again ridge regression using k = 0.0033
```{r}
mod22rn = lmridge(y~., as.data.frame(b.3), 0.0033)
vif(mod22rn)
coef(mod22rn, 0.0033)
coef(pcr_model)
#The coefficients are significantly reduced in ridge regression compared to PCR. $R^2$ is ridge is 0.8133.
```
```{r}
rstats1(mod22rn)
#$R^2$ is PCR is 75.73. $R^2$ is less in PCR compared to ridge. The coefficient is significantly reduced in Ridge as compared to PCR and OLS.
```
